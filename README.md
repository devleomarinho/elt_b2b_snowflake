*Languages available: [English](README.md), [PortuguÃªs](README.pt-br.md).*

---

# ELT Pipeline for ERP and CRM Data with Snowflake and GCP

![GCP](https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)

This repository contains an end-to-end Data Engineering project focused on B2B Commercial Intelligence and Revenue Operations (RevOps). The pipeline implements a Data Lakehouse architecture using Snowflake as the central platform, designed to process structured and semi-structured data in a scalable and compute-cost-optimized manner.

## Project Overview
B2B companies frequently face the challenge of crossing traditional transactional data (goals, costs, product catalogs) from ERPs with highly nested data generated by modern CRMs. Traditionally, processing complex JSON files requires separate infrastructures (such as Spark clusters or heavy Python scripts), which increases data latency and maintenance complexity. In addition, ingestion strategies that reprocess the entire data volume in each cycle generate excessive computational costs and slow down report delivery.

### Solution and Benefits
This project solves these problems by building a 100% cloud and managed Data Warehouse. The main benefits achieved are:

**Complexity Reduction**: Extraction and flattening of JSON data directly in the database layer using pure SQL.

**Cost Optimization (FinOps)**: Implementation of an idempotent incremental architecture. The database identifies and updates only the records that have undergone real changes, saving processing credits.

**Reliability (Data Quality)**: Strict typing and deduplication rules applied natively, ensuring that Business Intelligence dashboards consume complete and validated information.

**Ease of Maintenance:** The pipeline was built in Snowflake using SQL, a language universally used by data professionals, which facilitates the maintenance and scalability of the project.

### Technical Engineering Highlights

To achieve the proposed objectives, the project makes extensive use of advanced SQL features and Snowflake's compute engine:

**Change Data Capture (CDC) with Hash Diff (MD5)** The update of the analytical layer (Gold) was built to be completely incremental and idempotent. Instead of comparing columns individually or performing full loads, the project uses the MERGE statement combined with the MD5(OBJECT_CONSTRUCT(*)::VARCHAR) function. This approach packages the entire source row into a JSON object and generates a unique hash. If the source hash differs from the destination hash, the record is updated; otherwise, it is ignored. This represents the gold standard in Slowly Changing Dimension (SCD) modeling.

**Native JSON Processing (Semi-structured):** Raw CRM data enters the database in VARIANT columns. In the transformation layer, the LATERAL FLATTEN function is used to iterate over arrays and explode JSON objects into relational rows and columns. This made it possible to extract custom fields and product lists that were deeply nested in the file structure, eliminating external pre-processing tools.

**Quality Assurance and Deduplication:** To prevent the propagation of duplicates and inconsistencies, the pipeline uses the QUALIFY ROW_NUMBER() OVER(...) = 1 statement directly in the transformation scripts, ensuring that only the latest version of an event advances to the final layers, even in cases of systemic failures at the source.

## Solution Architecture

<img width="6235" height="1815" alt="diagram" src="https://github.com/user-attachments/assets/99829557-3ebf-4505-95e0-d453d305fe1f" />


The project follows the Medallion Architecture adapted for the Snowflake environment:

**Ingestion (Extract):** Transactional CSV files (ERP) and JSON API responses (CRM) are stored in Google Cloud Storage (GCS).

**Secure Integration (Storage Integration):** Snowflake establishes a trust-based connection via IAM Roles with GCP, allowing files to be read without exposing static access keys.

**Bronze Layer (Raw):** Raw tables populated via COPY INTO. Data remains in its original format (STRING or VARIANT), preserving the extraction history and structural metadata.

**Silver Layer (Clean):** Type conversion via TRY_CAST, string cleaning with REGEXP_REPLACE, list handling, JSON flattening, and deterministic deduplication.

**Gold Layer (Curated):** Dimensional modeling based on the Star Schema model. Fact and Dimension tables are updated incrementally, ready for visualization tools.

### Technologies Used

- Cloud Storage: Google Cloud Storage (Data Lake)

- Data Platform / Data Warehouse: Snowflake

- Transformation Language: SQL (Snowflake Dialect)

- Modeling: Star Schema, Medallion Architecture

### Data Modeling
The final Data Mart delivers the following main tables for consumption:

| Table | Type | Description |
| :--- | :--- | :--- |
| F_VENDAS | Fact | Transaction oportunities from the CRM (Pipeline). |
| F_METAS | Fact | Monthly sales goals per salesperson. |
| F_MARKETING | Fact | Leads acquisition costs per channel. |
| F_VENDAS_ITENS | Bridge | Link table for analysing multiples products per sale. |
| DIM_CLIENTES | Dimension | Demographic data of the companies (location, size, sector). |
| DIM_PRODUTOS | Dimension | Products and services catalog with price tags. |
| DIM_VENDEDORES | Dimension | Sales team (SCT type 1). |

### Contact
Project developed by Leonardo Marinho.
[LinkedIn](https://www.linkedin.com/in/devleomarinho/) | [Email](mailto:dev.leomarinho@gmail.com)
